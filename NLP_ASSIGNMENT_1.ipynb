{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_ASSIGNMENT-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOd3r59c3IG5ec0PTqkPp9R"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYmmtM-jtico"
      },
      "source": [
        "##Solving  an  NLP  problem  is  a  multi-stage  process.  We  need  to  clean  the  unstructured  text  data  first before we can even think about getting to the modeling stage. Cleaning the data consists of a few key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71Fr6i3rtzES"
      },
      "source": [
        "## 1.Split the above paragraph into sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozXfTsegreAk",
        "outputId": "0df731ef-8acd-40cd-d623-cf111b2066d6"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize , word_tokenize\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yT1EZ3kRueen",
        "outputId": "62a27022-8978-4546-c84a-a84d975a65fd"
      },
      "source": [
        "para_text = \"Solving  an  NLP  problem  is  a  multi-stage  process.  We  need  to  clean  the  unstructured  text  data  first before we can even think about getting to the modeling stage. Cleaning the data consists of a few key.\"\n",
        "print(sent_tokenize(para_text))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Solving  an  NLP  problem  is  a  multi-stage  process.', 'We  need  to  clean  the  unstructured  text  data  first before we can even think about getting to the modeling stage.', 'Cleaning the data consists of a few key.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBO-DR1iystM"
      },
      "source": [
        "##2.split the above paragraph into words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYq6b5WawzXr",
        "outputId": "83ff643f-b16c-4195-ea1b-182db79ad6da"
      },
      "source": [
        "print(word_tokenize(para_text))\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Solving', 'an', 'NLP', 'problem', 'is', 'a', 'multi-stage', 'process', '.', 'We', 'need', 'to', 'clean', 'the', 'unstructured', 'text', 'data', 'first', 'before', 'we', 'can', 'even', 'think', 'about', 'getting', 'to', 'the', 'modeling', 'stage', '.', 'Cleaning', 'the', 'data', 'consists', 'of', 'a', 'few', 'key', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh0txMz7zK0X"
      },
      "source": [
        "##3. Find stem and lemma words for the given words?\n",
        "“cats\"\n",
        "\"trouble\"\n",
        "\"troubling\"\n",
        "\"troubled\"\n",
        "“having”\n",
        "“Corriendo”\n",
        "“at”\n",
        "“was”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pt31lZRa35Ye"
      },
      "source": [
        "##(A)stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BWgSmuH0cBX",
        "outputId": "53994138-e5cc-4f76-a8ed-761e6dfe5e8c"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "Words =[\"cats\",\"trouble\",\"troubling\",\"troubled\",\"having\",\"Corriendo\",\"at\",\"was\"]\n",
        "for i in Words:\n",
        "  print(ps.stem(i))\n",
        "  "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "have\n",
            "corriendo\n",
            "at\n",
            "wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkv3yir94AXq"
      },
      "source": [
        "##(B) lemmatizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1vz-hY_6UIA",
        "outputId": "3e5b27a0-e9b5-4e8a-e16f-a6f96afa0713"
      },
      "source": [
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "Words = [\"cats\",\"trouble\",\"troubling\",\"troubled\",\"having\",\"Corriendo\",\"at\",\"was\"]\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "for w in Words:\n",
        "   print(lemmatizer.lemmatize(w))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "cat\n",
            "trouble\n",
            "troubling\n",
            "troubled\n",
            "having\n",
            "Corriendo\n",
            "at\n",
            "wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVEfhf6r2cXY"
      },
      "source": [
        "##4. Find stop words from the given paragraph?\n",
        "##“The NLTK library  is  one  of  the  oldest  and  most  commonly  used  Python  libraries  for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the  corpus  module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9RFdHqZ2r9I",
        "outputId": "52901c0c-ef85-479d-97fd-708273004f8b"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "para_2 = \"The NLTK library  is  one  of  the  oldest  and  most  commonly  used  Python  libraries  for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the  corpus  module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\"\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "word_list = word_tokenize(para_2)\n",
        "for i in stop_words:\n",
        "  print(i)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "she\n",
            "its\n",
            "all\n",
            "ve\n",
            "no\n",
            "won't\n",
            "had\n",
            "our\n",
            "where\n",
            "your\n",
            "whom\n",
            "aren't\n",
            "haven't\n",
            "and\n",
            "by\n",
            "my\n",
            "from\n",
            "up\n",
            "them\n",
            "but\n",
            "mustn't\n",
            "her\n",
            "into\n",
            "of\n",
            "off\n",
            "themselves\n",
            "to\n",
            "after\n",
            "each\n",
            "few\n",
            "have\n",
            "during\n",
            "itself\n",
            "we\n",
            "below\n",
            "needn\n",
            "above\n",
            "shouldn't\n",
            "did\n",
            "there\n",
            "weren't\n",
            "should\n",
            "here\n",
            "y\n",
            "isn\n",
            "t\n",
            "can\n",
            "yourselves\n",
            "further\n",
            "me\n",
            "under\n",
            "same\n",
            "theirs\n",
            "hadn\n",
            "down\n",
            "s\n",
            "mustn\n",
            "more\n",
            "because\n",
            "having\n",
            "are\n",
            "been\n",
            "own\n",
            "this\n",
            "do\n",
            "you\n",
            "she's\n",
            "ain\n",
            "for\n",
            "doing\n",
            "will\n",
            "didn't\n",
            "at\n",
            "about\n",
            "re\n",
            "it's\n",
            "shan\n",
            "shouldn\n",
            "does\n",
            "over\n",
            "an\n",
            "wouldn't\n",
            "you'll\n",
            "which\n",
            "were\n",
            "it\n",
            "ll\n",
            "some\n",
            "o\n",
            "wouldn\n",
            "needn't\n",
            "or\n",
            "between\n",
            "d\n",
            "hasn't\n",
            "as\n",
            "both\n",
            "against\n",
            "with\n",
            "most\n",
            "just\n",
            "is\n",
            "then\n",
            "what\n",
            "very\n",
            "too\n",
            "don't\n",
            "mightn't\n",
            "couldn\n",
            "am\n",
            "myself\n",
            "through\n",
            "m\n",
            "yours\n",
            "if\n",
            "you're\n",
            "doesn\n",
            "his\n",
            "didn\n",
            "their\n",
            "hers\n",
            "that\n",
            "ours\n",
            "yourself\n",
            "they\n",
            "out\n",
            "wasn\n",
            "him\n",
            "herself\n",
            "aren\n",
            "why\n",
            "how\n",
            "such\n",
            "these\n",
            "now\n",
            "than\n",
            "so\n",
            "isn't\n",
            "be\n",
            "you'd\n",
            "before\n",
            "when\n",
            "nor\n",
            "not\n",
            "hadn't\n",
            "i\n",
            "only\n",
            "other\n",
            "haven\n",
            "ma\n",
            "should've\n",
            "shan't\n",
            "that'll\n",
            "himself\n",
            "won\n",
            "has\n",
            "you've\n",
            "hasn\n",
            "doesn't\n",
            "was\n",
            "again\n",
            "those\n",
            "weren\n",
            "mightn\n",
            "wasn't\n",
            "couldn't\n",
            "the\n",
            "who\n",
            "don\n",
            "he\n",
            "ourselves\n",
            "while\n",
            "until\n",
            "on\n",
            "any\n",
            "being\n",
            "once\n",
            "in\n",
            "a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cswI5avR9BLu"
      },
      "source": [
        "##5. From the above paragraph print frequency of each word using NLTK?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lz7HHbtLCQoZ",
        "outputId": "b94b73f6-b902-46be-8ceb-f5f9a5218f08"
      },
      "source": [
        "para_2 = \"The NLTK library  is  one  of  the  oldest  and  most  commonly  used  Python  libraries  for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the  corpus  module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\"\n",
        "words = para_2.split()\n",
        "d = {}\n",
        "for i in words:\n",
        "   if i not in d.keys():\n",
        "     d[i]=0\n",
        "   d[i]=d[i]+1\n",
        "print(d)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'The': 1, 'NLTK': 2, 'library': 1, 'is': 1, 'one': 1, 'of': 3, 'the': 5, 'oldest': 1, 'and': 3, 'most': 1, 'commonly': 1, 'used': 1, 'Python': 1, 'libraries': 1, 'for': 1, 'Natural': 1, 'Language': 1, 'Processing.': 1, 'supports': 1, 'stop': 4, 'word': 2, 'removal,': 1, 'you': 2, 'can': 2, 'find': 1, 'list': 2, 'words': 4, 'in': 2, 'corpus': 1, 'module.': 1, 'To': 1, 'remove': 2, 'from': 1, 'a': 1, 'sentence,': 1, 'divide': 1, 'your': 1, 'text': 1, 'into': 1, 'then': 1, 'if': 1, 'it': 1, 'exits': 1, 'provided': 1, 'by': 1, 'NLTK.': 1}\n"
          ]
        }
      ]
    }
  ]
}